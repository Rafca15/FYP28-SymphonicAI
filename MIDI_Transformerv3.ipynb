{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ePs54M4SnMtl",
        "outputId": "b08c7e71-7e21-4335-bb02-b422c5cfa33a"
      },
      "outputs": [],
      "source": [
        "!7z x Dataset.7z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osmP-Y3qpPBh",
        "outputId": "fd9fefdb-8fb4-46cd-ad33-058f933447e0"
      },
      "outputs": [],
      "source": [
        "!pip install miditoolkit mido pandas seaborn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t49bgeTRpaBL"
      },
      "outputs": [],
      "source": [
        "import miditoolkit\n",
        "from mido import MidiFile, MidiTrack, MetaMessage, Message\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4b4RKcAdpo9X"
      },
      "outputs": [],
      "source": [
        "def extract_note_attributes(note):\n",
        "    duration = max(60, round((note.end - note.start) / 60) * 60)\n",
        "    return (note.pitch, note.start, duration)\n",
        "\n",
        "\n",
        "def insert_silence_tokens(note_events):\n",
        "    result = []\n",
        "    prev_end = None\n",
        "\n",
        "    for pitch, start, duration in note_events:\n",
        "        if prev_end is not None and start > prev_end:\n",
        "            silence_duration = max(60, round((start - prev_end) / 60) * 60)\n",
        "            result.append((0, silence_duration))\n",
        "\n",
        "        result.append((pitch, duration))\n",
        "        prev_end = start + duration\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def extract_metadata(filename):\n",
        "    midi = MidiFile(filename)\n",
        "    metadata = {\n",
        "        'ticks_per_beat': midi.ticks_per_beat,\n",
        "        'time_signature': MetaMessage('time_signature', numerator=4, denominator=4, clocks_per_click=24, notated_32nd_notes_per_beat=8),\n",
        "        'key_signature': MetaMessage('key_signature', key='C'),\n",
        "        'set_tempo': MetaMessage('set_tempo', tempo=500000)  # Default 120 BPM\n",
        "    }\n",
        "\n",
        "    for track in midi.tracks:\n",
        "        for msg in track:\n",
        "            if msg.type in metadata:\n",
        "                metadata[msg.type] = msg\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def encode_midi(filename):\n",
        "    midi = miditoolkit.MidiFile(filename)\n",
        "    note_events = [\n",
        "        extract_note_attributes(note)\n",
        "        for instrument in midi.instruments\n",
        "        for note in instrument.notes\n",
        "    ]\n",
        "\n",
        "    tokens = insert_silence_tokens(note_events)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def tokens_to_ids(tokens, token_dict):\n",
        "    return [token_dict[token] for token in tokens]\n",
        "\n",
        "\n",
        "def ids_to_tokens(ids, id_to_token):\n",
        "    return [id_to_token[_id] for _id in ids]\n",
        "\n",
        "\n",
        "def decode_midi(notes, output_filename, metadata):\n",
        "    mid = MidiFile(ticks_per_beat=metadata['ticks_per_beat'])\n",
        "    track = MidiTrack()\n",
        "    mid.tracks.append(track)\n",
        "\n",
        "    # Add metadata and instrument info\n",
        "    track.extend([\n",
        "        metadata['time_signature'],\n",
        "        metadata['key_signature'],\n",
        "        metadata['set_tempo'],\n",
        "        Message('program_change', channel=0, program=1, time=0),  # Instrument\n",
        "        Message('control_change', channel=0, control=7, value=80, time=0)  # Volume\n",
        "    ])\n",
        "\n",
        "    current_time = 0\n",
        "    for pitch, duration in notes:\n",
        "\n",
        "        if pitch == 0:\n",
        "            current_time += duration\n",
        "        else:\n",
        "            track.append(Message('note_on', note=pitch, velocity=100, time=current_time))\n",
        "            track.append(Message('note_on', note=pitch, velocity=0, time=duration))\n",
        "            current_time = 0\n",
        "\n",
        "    mid.save(output_filename)\n",
        "\n",
        "\n",
        "def decode_ids_to_midi(id_sequence, output_filename, metadata, id_to_token):\n",
        "    tokens = ids_to_tokens(id_sequence, id_to_token)\n",
        "    decode_midi(tokens, output_filename, metadata)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HvSCU-6-UH6E",
        "outputId": "3e7bfc4e-6ef2-4c26-a1a1-2d50529d4cea"
      },
      "outputs": [],
      "source": [
        "DATASET_DIR = \"/content/Dataset\"\n",
        "all_tokens = []\n",
        "token_set = set()\n",
        "\n",
        "for filename in os.listdir(DATASET_DIR):\n",
        "    if filename.lower().endswith('.mid'):\n",
        "        filepath = os.path.join(DATASET_DIR, filename)\n",
        "        tokens = encode_midi(filepath)\n",
        "\n",
        "        all_tokens.extend(tokens)\n",
        "        token_set.update(tokens)\n",
        "\n",
        "        print(f\"Encoded {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMsKdM5fVhyw",
        "outputId": "4d14a072-b4aa-4c18-f582-08e85bcde80a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1641743\n"
          ]
        }
      ],
      "source": [
        "print(len(all_tokens)) # 1641743 tokens in vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so2k3nYBUMoc",
        "outputId": "91838495-7e7c-4285-f8ff-233397c46e39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(36, 480), (69, 1560), (60, 1740), (51, 1920), (0, 2220), (44, 420), (0, 13740), (81, 6000), (71, 2280), (0, 59640), (79, 8520), (0, 36600), (86, 3720), (67, 180), (58, 360), (70, 2400), (75, 300), (83, 6540), (66, 480), (52, 2760), (57, 660), (0, 9660), (0, 21180), (0, 32700), (48, 840), (81, 1920), (39, 1020), (72, 2100), (84, 2340), (59, 1380), (56, 780), (45, 3660), (67, 7620), (47, 960), (50, 1560), (83, 2640), (36, 3840), (74, 2820), (0, 5760), (76, 120), (88, 360), (79, 540), (82, 2760), (40, 1860), (78, 840), (67, 3720), (69, 1020), (60, 1200), (51, 1380), (0, 1680)]\n",
            "2788\n"
          ]
        }
      ],
      "source": [
        "print(list(token_set)[:50]) # vocab in tuples\n",
        "print(len(token_set)) # vocab size = 2788"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Vy7EHsDdSKd",
        "outputId": "d933d019-10e2-4ce2-9d82-4b053bd85176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[((0, 60), 0), ((0, 120), 1), ((0, 180), 2), ((0, 240), 3), ((0, 300), 4), ((0, 360), 5), ((0, 420), 6), ((0, 480), 7), ((0, 540), 8), ((0, 600), 9), ((0, 660), 10), ((0, 720), 11), ((0, 780), 12), ((0, 840), 13), ((0, 900), 14), ((0, 960), 15), ((0, 1020), 16), ((0, 1080), 17), ((0, 1140), 18), ((0, 1200), 19), ((0, 1260), 20), ((0, 1320), 21), ((0, 1380), 22), ((0, 1440), 23), ((0, 1500), 24), ((0, 1560), 25), ((0, 1620), 26), ((0, 1680), 27), ((0, 1740), 28), ((0, 1800), 29), ((0, 1860), 30), ((0, 1920), 31), ((0, 1980), 32), ((0, 2040), 33), ((0, 2100), 34), ((0, 2160), 35), ((0, 2220), 36), ((0, 2280), 37), ((0, 2340), 38), ((0, 2400), 39), ((0, 2460), 40), ((0, 2520), 41), ((0, 2580), 42), ((0, 2640), 43), ((0, 2700), 44), ((0, 2760), 45), ((0, 2820), 46), ((0, 2880), 47), ((0, 2940), 48), ((0, 3000), 49)]\n",
            "2788\n"
          ]
        }
      ],
      "source": [
        "token_dict = {token: idx for idx, token in enumerate(sorted(token_set))}\n",
        "print(list(token_dict.items())[:50])\n",
        "print(len(token_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qe1CrBwVgPGV"
      },
      "outputs": [],
      "source": [
        "# Create token-to-ID mapping\n",
        "token2id = {token: idx for idx, token in enumerate(sorted(token_set))}\n",
        "id2token = {idx: token for token, idx in token2id.items()}  # Optional: for decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "x6SG6IUAgSR0"
      },
      "outputs": [],
      "source": [
        "all_token_ids = tokens_to_ids(all_tokens, token2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgzL5lZGgVjR",
        "outputId": "c2211544-afed-4f01-d9b9-c28be870eb40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1641743\n",
            "[2372, 0, 2464, 0, 2370, 0, 2462, 0, 2276, 0, 2462, 0, 2372, 0, 2011, 0, 2222, 0, 2278, 0, 2220, 0, 2276, 0, 2107, 0, 2276, 0, 2227, 2372, 0, 2464, 0, 2370, 0, 2462, 0, 2276, 0, 2462, 0, 2372, 0, 2614, 0, 2517, 0, 2464, 0, 2553]\n"
          ]
        }
      ],
      "source": [
        "print(len(all_token_ids)) # All tokens from their IDs\n",
        "print(all_token_ids[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wz2bAM7hJvi",
        "outputId": "238000cd-3b48-4fdb-e273-2ceeeea1d106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1641743]) torch.int64\n",
            "tensor([2372,    0, 2464,    0, 2370,    0, 2462,    0, 2276,    0, 2462,    0,\n",
            "        2372,    0, 2011,    0, 2222,    0, 2278,    0, 2220,    0, 2276,    0,\n",
            "        2107,    0, 2276,    0, 2227, 2372,    0, 2464,    0, 2370,    0, 2462,\n",
            "           0, 2276,    0, 2462,    0, 2372,    0, 2614,    0, 2517,    0, 2464,\n",
            "           0, 2553,    0, 2614,    0, 2558, 2674,    0, 2674,    0, 2464,    0,\n",
            "        2614,    0, 2551,    0, 2612,    0, 2551,    0, 2462,    0, 2370,    0,\n",
            "        2329,    0, 2222,    0, 2464,    0, 2222,    0, 2331,    0, 2220,    0,\n",
            "        2107,    0, 2372,    0, 2011,    0, 2011,    0, 2011,    0, 2011,    0,\n",
            "        1958,    0, 1846,    0])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(all_token_ids, dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-uWgKxRs1K9"
      },
      "source": [
        "Splitting MIDI files into training and validation sets with a 0.1:0.9 ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_-RMDf0gvDC",
        "outputId": "bc77fa28-251b-4b4a-c6f2-9aa8ec06e2a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1477568\n",
            "164175\n"
          ]
        }
      ],
      "source": [
        "# split the data into training and testing data with a ration of 0.9:0.1\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]\n",
        "\n",
        "print(len(train_data)) # 0.9 * 1641743 = 1477568\n",
        "print(len(test_data)) # 0.1 * 1641743 = 164175"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZd0zoWk1Fdt"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byKeeLWy2Jdg"
      },
      "outputs": [],
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else test_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQodcBPVuQOg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MIDITokenDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx : idx + self.block_size]\n",
        "        y = self.data[idx + 1 : idx + 1 + self.block_size]\n",
        "        return {\"input_ids\": x, \"labels\": y}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cae53DHKuVeI"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "block_size = 128\n",
        "batch_size = 32\n",
        "n_embd = 128\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.1\n",
        "\n",
        "eval_iters = 200\n",
        "\n",
        "train_dataset = MIDITokenDataset(train_data, block_size=block_size)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yzWIMMhu8Nw"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size, n_embd, block_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "        wei = q @ k.transpose(-2, -1) / (k.size(-1) ** 0.5)\n",
        "\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        return self.dropout(wei @ v)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, n_embd, block_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // num_heads\n",
        "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.proj(torch.cat([h(x) for h in self.heads], dim=-1)))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(n_head, n_embd, block_size, dropout)\n",
        "        self.ffwd = FeedForward(n_embd, dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TransformerMIDILanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd=128, n_head=4, n_layer=6, block_size=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
        "\n",
        "        x = self.blocks(tok_emb + pos_emb)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1), ignore_index=-100)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, eos_token_id=None):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "            if eos_token_id is not None and idx_next.item() == eos_token_id:\n",
        "                break\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhc7LzMo219y",
        "outputId": "67b1f5a1-1dfb-47c9-e480-3dec22acf7cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.524708 M parameters\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(token2id)\n",
        "\n",
        "model = TransformerMIDILanguageModel(\n",
        "    vocab_size=vocab_size,\n",
        "    n_embd=n_embd,\n",
        "    n_head=n_head,\n",
        "    n_layer=n_layer,\n",
        "    block_size=block_size,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TAs0gqU3Lqe"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEM6sVLy3dtd",
        "outputId": "84f5889f-002f-464c-b77d-8dc543dc18b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 3.3903, val loss 3.4575\n",
            "step 100: train loss 3.3839, val loss 3.4364\n",
            "step 200: train loss 3.3597, val loss 3.4281\n",
            "step 300: train loss 3.3590, val loss 3.4236\n",
            "step 400: train loss 3.3357, val loss 3.4017\n",
            "step 500: train loss 3.3273, val loss 3.3985\n",
            "step 600: train loss 3.3070, val loss 3.3865\n",
            "step 700: train loss 3.2937, val loss 3.3872\n",
            "step 800: train loss 3.2933, val loss 3.3660\n",
            "step 900: train loss 3.2923, val loss 3.3597\n",
            "step 1000: train loss 3.2810, val loss 3.3353\n",
            "step 1100: train loss 3.2685, val loss 3.3419\n",
            "step 1200: train loss 3.2644, val loss 3.3359\n",
            "step 1300: train loss 3.2531, val loss 3.3153\n",
            "step 1400: train loss 3.2480, val loss 3.3177\n",
            "step 1500: train loss 3.2180, val loss 3.3188\n",
            "step 1600: train loss 3.2186, val loss 3.3151\n",
            "step 1700: train loss 3.2235, val loss 3.2924\n",
            "step 1800: train loss 3.2229, val loss 3.2807\n",
            "step 1900: train loss 3.1973, val loss 3.2842\n",
            "step 2000: train loss 3.1988, val loss 3.2885\n",
            "step 2100: train loss 3.2044, val loss 3.2668\n",
            "step 2200: train loss 3.1821, val loss 3.2648\n",
            "step 2300: train loss 3.1789, val loss 3.2455\n",
            "step 2400: train loss 3.1533, val loss 3.2569\n",
            "step 2500: train loss 3.1695, val loss 3.2473\n",
            "step 2600: train loss 3.1498, val loss 3.2519\n",
            "step 2700: train loss 3.1531, val loss 3.2340\n",
            "step 2800: train loss 3.1316, val loss 3.2418\n",
            "step 2900: train loss 3.1373, val loss 3.2231\n",
            "step 3000: train loss 3.1306, val loss 3.2307\n",
            "step 3100: train loss 3.1263, val loss 3.2252\n",
            "step 3200: train loss 3.1158, val loss 3.2146\n",
            "step 3300: train loss 3.1159, val loss 3.2058\n",
            "step 3400: train loss 3.1070, val loss 3.1999\n",
            "step 3500: train loss 3.1106, val loss 3.1990\n",
            "step 3600: train loss 3.1206, val loss 3.1881\n",
            "step 3700: train loss 3.1041, val loss 3.1852\n",
            "step 3800: train loss 3.1012, val loss 3.1804\n",
            "step 3900: train loss 3.0806, val loss 3.1786\n",
            "step 4000: train loss 3.1011, val loss 3.1811\n",
            "step 4100: train loss 3.0872, val loss 3.1764\n",
            "step 4200: train loss 3.0653, val loss 3.1546\n",
            "step 4300: train loss 3.0747, val loss 3.1621\n",
            "step 4400: train loss 3.0654, val loss 3.1604\n",
            "step 4500: train loss 3.0770, val loss 3.1654\n",
            "step 4600: train loss 3.0574, val loss 3.1618\n",
            "step 4700: train loss 3.0507, val loss 3.1540\n",
            "step 4800: train loss 3.0538, val loss 3.1518\n",
            "step 4900: train loss 3.0551, val loss 3.1435\n",
            "step 5000: train loss 3.0429, val loss 3.1389\n",
            "step 5100: train loss 3.0558, val loss 3.1452\n",
            "step 5200: train loss 3.0323, val loss 3.1464\n",
            "step 5300: train loss 3.0362, val loss 3.1436\n",
            "step 5400: train loss 3.0239, val loss 3.1501\n",
            "step 5500: train loss 3.0249, val loss 3.1278\n",
            "step 5600: train loss 3.0170, val loss 3.1350\n",
            "step 5700: train loss 3.0280, val loss 3.1345\n",
            "step 5800: train loss 3.0146, val loss 3.1156\n",
            "step 5900: train loss 3.0054, val loss 3.1147\n",
            "step 6000: train loss 3.0172, val loss 3.1124\n",
            "step 6100: train loss 3.0183, val loss 3.1157\n",
            "step 6200: train loss 3.0143, val loss 3.1202\n",
            "step 6300: train loss 3.0005, val loss 3.1219\n",
            "step 6400: train loss 2.9976, val loss 3.1094\n",
            "step 6500: train loss 3.0000, val loss 3.1050\n",
            "step 6600: train loss 3.0017, val loss 3.0874\n",
            "step 6700: train loss 2.9939, val loss 3.1176\n",
            "step 6800: train loss 2.9892, val loss 3.0884\n",
            "step 6900: train loss 2.9981, val loss 3.1075\n",
            "step 7000: train loss 2.9753, val loss 3.0961\n",
            "step 7100: train loss 2.9836, val loss 3.0955\n",
            "step 7200: train loss 2.9741, val loss 3.0967\n",
            "step 7300: train loss 2.9743, val loss 3.0843\n",
            "step 7400: train loss 2.9778, val loss 3.0662\n",
            "step 7500: train loss 2.9624, val loss 3.0863\n",
            "step 7600: train loss 2.9619, val loss 3.0673\n",
            "step 7700: train loss 2.9734, val loss 3.0739\n",
            "step 7800: train loss 2.9621, val loss 3.0658\n",
            "step 7900: train loss 2.9575, val loss 3.0664\n",
            "step 8000: train loss 2.9572, val loss 3.0773\n",
            "step 8100: train loss 2.9699, val loss 3.0600\n",
            "step 8200: train loss 2.9370, val loss 3.0637\n",
            "step 8300: train loss 2.9439, val loss 3.0693\n",
            "step 8400: train loss 2.9568, val loss 3.0630\n",
            "step 8500: train loss 2.9439, val loss 3.0632\n",
            "step 8600: train loss 2.9387, val loss 3.0488\n",
            "step 8700: train loss 2.9432, val loss 3.0565\n",
            "step 8800: train loss 2.9316, val loss 3.0604\n",
            "step 8900: train loss 2.9284, val loss 3.0423\n",
            "step 9000: train loss 2.9244, val loss 3.0478\n",
            "step 9100: train loss 2.9356, val loss 3.0388\n",
            "step 9200: train loss 2.9371, val loss 3.0517\n",
            "step 9300: train loss 2.9149, val loss 3.0559\n",
            "step 9400: train loss 2.9180, val loss 3.0439\n",
            "step 9500: train loss 2.9015, val loss 3.0430\n",
            "step 9600: train loss 2.9114, val loss 3.0498\n",
            "step 9700: train loss 2.9149, val loss 3.0356\n",
            "step 9800: train loss 2.9273, val loss 3.0485\n",
            "step 9900: train loss 2.9025, val loss 3.0243\n",
            "step 10000: train loss 2.9115, val loss 3.0284\n",
            "step 10100: train loss 2.9059, val loss 3.0555\n",
            "step 10200: train loss 2.8858, val loss 3.0294\n",
            "step 10300: train loss 2.8970, val loss 3.0242\n",
            "step 10400: train loss 2.9159, val loss 3.0161\n",
            "step 10500: train loss 2.9106, val loss 3.0108\n",
            "step 10600: train loss 2.8908, val loss 3.0053\n",
            "step 10700: train loss 2.8854, val loss 3.0136\n",
            "step 10800: train loss 2.8950, val loss 3.0146\n",
            "step 10900: train loss 2.8815, val loss 2.9943\n",
            "step 11000: train loss 2.8956, val loss 3.0123\n",
            "step 11100: train loss 2.8773, val loss 3.0054\n",
            "step 11200: train loss 2.8936, val loss 3.0120\n",
            "step 11300: train loss 2.8832, val loss 2.9930\n",
            "step 11400: train loss 2.8822, val loss 3.0079\n",
            "step 11500: train loss 2.8694, val loss 3.0208\n",
            "step 11600: train loss 2.8723, val loss 2.9822\n",
            "step 11700: train loss 2.8671, val loss 2.9986\n",
            "step 11800: train loss 2.8693, val loss 3.0019\n",
            "step 11900: train loss 2.8640, val loss 2.9887\n",
            "step 12000: train loss 2.8679, val loss 3.0045\n",
            "step 12100: train loss 2.8672, val loss 3.0015\n",
            "step 12200: train loss 2.8628, val loss 2.9944\n",
            "step 12300: train loss 2.8517, val loss 2.9955\n",
            "step 12400: train loss 2.8551, val loss 2.9834\n",
            "step 12500: train loss 2.8563, val loss 2.9883\n",
            "step 12600: train loss 2.8541, val loss 2.9921\n",
            "step 12700: train loss 2.8452, val loss 2.9894\n",
            "step 12800: train loss 2.8537, val loss 2.9759\n",
            "step 12900: train loss 2.8441, val loss 2.9800\n",
            "step 13000: train loss 2.8368, val loss 2.9764\n",
            "step 13100: train loss 2.8481, val loss 2.9681\n",
            "step 13200: train loss 2.8433, val loss 2.9701\n",
            "step 13300: train loss 2.8338, val loss 2.9819\n",
            "step 13400: train loss 2.8328, val loss 2.9535\n",
            "step 13500: train loss 2.8272, val loss 2.9811\n",
            "step 13600: train loss 2.8203, val loss 2.9615\n",
            "step 13700: train loss 2.8202, val loss 2.9573\n",
            "step 13800: train loss 2.8368, val loss 2.9605\n",
            "step 13900: train loss 2.8346, val loss 2.9546\n",
            "step 14000: train loss 2.8276, val loss 2.9653\n",
            "step 14100: train loss 2.8211, val loss 2.9657\n",
            "step 14200: train loss 2.8150, val loss 2.9515\n",
            "step 14300: train loss 2.8050, val loss 2.9653\n",
            "step 14400: train loss 2.8075, val loss 2.9667\n",
            "step 14500: train loss 2.8198, val loss 2.9436\n",
            "step 14600: train loss 2.8076, val loss 2.9418\n",
            "step 14700: train loss 2.8019, val loss 2.9496\n",
            "step 14800: train loss 2.8090, val loss 2.9428\n",
            "step 14900: train loss 2.8054, val loss 2.9587\n",
            "step 15000: train loss 2.8058, val loss 2.9259\n",
            "step 15100: train loss 2.7921, val loss 2.9275\n",
            "step 15200: train loss 2.7981, val loss 2.9426\n",
            "step 15300: train loss 2.7916, val loss 2.9205\n",
            "step 15400: train loss 2.7976, val loss 2.9377\n",
            "step 15500: train loss 2.7898, val loss 2.9371\n",
            "step 15600: train loss 2.7915, val loss 2.9144\n",
            "step 15700: train loss 2.7811, val loss 2.9297\n",
            "step 15800: train loss 2.7867, val loss 2.9311\n",
            "step 15900: train loss 2.7779, val loss 2.9272\n",
            "step 16000: train loss 2.7807, val loss 2.9406\n",
            "step 16100: train loss 2.7753, val loss 2.9065\n",
            "step 16200: train loss 2.7747, val loss 2.9288\n",
            "step 16300: train loss 2.7672, val loss 2.9123\n",
            "step 16400: train loss 2.7713, val loss 2.9284\n",
            "step 16500: train loss 2.7691, val loss 2.9162\n",
            "step 16600: train loss 2.7599, val loss 2.9276\n",
            "step 16700: train loss 2.7610, val loss 2.9248\n",
            "step 16800: train loss 2.7565, val loss 2.9194\n",
            "step 16900: train loss 2.7618, val loss 2.9098\n",
            "step 17000: train loss 2.7527, val loss 2.9095\n",
            "step 17100: train loss 2.7533, val loss 2.8920\n",
            "step 17200: train loss 2.7542, val loss 2.9000\n",
            "step 17300: train loss 2.7443, val loss 2.9092\n",
            "step 17400: train loss 2.7395, val loss 2.9063\n",
            "step 17500: train loss 2.7496, val loss 2.8952\n",
            "step 17600: train loss 2.7427, val loss 2.8904\n",
            "step 17700: train loss 2.7421, val loss 2.8986\n",
            "step 17800: train loss 2.7380, val loss 2.8884\n",
            "step 17900: train loss 2.7191, val loss 2.8889\n",
            "step 18000: train loss 2.7330, val loss 2.9095\n",
            "step 18100: train loss 2.7420, val loss 2.8940\n",
            "step 18200: train loss 2.7326, val loss 2.8854\n",
            "step 18300: train loss 2.7304, val loss 2.8757\n",
            "step 18400: train loss 2.7350, val loss 2.8923\n",
            "step 18500: train loss 2.7149, val loss 2.8620\n",
            "step 18600: train loss 2.7230, val loss 2.8843\n",
            "step 18700: train loss 2.7305, val loss 2.8789\n",
            "step 18800: train loss 2.7148, val loss 2.8754\n",
            "step 18900: train loss 2.7242, val loss 2.8735\n",
            "step 19000: train loss 2.7104, val loss 2.8698\n",
            "step 19100: train loss 2.7230, val loss 2.8843\n",
            "step 19200: train loss 2.7194, val loss 2.8768\n",
            "step 19300: train loss 2.7044, val loss 2.8598\n",
            "step 19400: train loss 2.7160, val loss 2.8755\n",
            "step 19500: train loss 2.7186, val loss 2.8765\n",
            "step 19600: train loss 2.6996, val loss 2.8642\n",
            "step 19700: train loss 2.6973, val loss 2.8597\n",
            "step 19800: train loss 2.6894, val loss 2.8707\n",
            "step 19900: train loss 2.7059, val loss 2.8508\n",
            "step 19999: train loss 2.7020, val loss 2.8412\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-4\n",
        "max_iters = 20000\n",
        "eval_interval = 100\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    _, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7rYpZ0d9TsX"
      },
      "outputs": [],
      "source": [
        "start_token = token2id[(0, 480)]  # or any common starting token\n",
        "context = torch.tensor([[start_token]], dtype=torch.long, device=device)\n",
        "\n",
        "generated_ids = model.generate(context, max_new_tokens=500)[0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl9q8nS09bF9",
        "outputId": "b47eab55-636e-4524-d7bb-45553aa1a8b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7, 2226, 2070, 2121, 2070, 2113, 2070, 2121, 2113, 2226, 2384, 2376, 2226, 2113, 2234, 2121, 2023, 1876, 2113, 2226, 2282, 2226, 2113, 2250, 23, 2230, 2278, 2222, 2109, 1964, 1852, 11, 2476, 2376, 2282, 1964, 11, 2464, 2372, 2276, 2226, 2113, 7, 2468, 2370, 2220, 2464, 2066, 2109, 2070, 2224, 2107, 1848, 1960, 2222, 2109, 2222, 2226, 2109, 1733, 2278, 2222, 2113, 2019, 2011, 2222, 2109, 1733, 1848, 2011, 1733, 1848, 1848, 2011, 2011, 1962, 1845, 1733, 7, 2222, 2276, 2370, 2220, 2109, 1852, 1964, 1733, 1848, 1960, 1735, 1731, 1850, 1846, 1960, 2011, 1958, 2064, 2107, 2220, 2109, 2109, 2226, 2113, 2226, 2431, 2278, 2226, 11, 1850, 1958, 2064, 2109, 2220, 2329, 2222, 1846, 2107, 2220, 2278, 2222, 2111, 2219, 2328, 2372, 2335, 3, 2113, 2066, 1846, 1958, 2066, 2109, 2374, 2220, 2278, 2224, 2107, 2015, 7, 1850, 1958, 2011, 1848, 2464, 2372, 2462, 2370, 1958, 2276, 2372, 2464, 2464, 2468, 3, 2372, 2224, 2276, 2109, 1850, 2009, 3, 2466, 2370, 2329, 2374, 2220, 2107, 2226, 1960, 1585, 2370, 2462, 2370, 2278, 0, 2515, 2466, 2370, 2331, 0, 2551, 2613, 2464, 0, 2462, 2614, 0, 2329, 2372, 0, 2462, 2464, 0, 2370, 2372, 0, 2462, 2517, 2372, 0, 2276, 2220, 2109, 0, 2220, 2220, 2298, 3, 2553, 2614, 2464, 0, 2107, 2222, 0, 2329, 2372, 0, 2462, 2464, 0, 2431, 0, 2331, 0, 2220, 2339, 2219, 0, 2107, 2222, 0, 1731, 1960, 0, 2011, 0, 2220, 2331, 0, 2329, 2392, 0, 2343, 2343, 10, 2542, 0, 2223, 0, 2110, 0, 2219, 0, 2063, 0, 2219, 0, 2219, 0, 2106, 0, 2372, 2371, 2011, 0, 2220, 0, 2464, 2221, 2329, 2577, 0, 2220, 2343, 0, 2370, 2276, 2222, 0, 2278, 2372, 5, 2282, 3, 2109, 1960, 2010, 0, 1848, 1960, 0, 1959, 0, 1847, 0, 1482, 0, 2111, 0, 1849, 1731, 2111, 0, 2064, 2111, 0, 2064, 2111, 0, 2065, 0, 2185, 0, 1849, 1731, 0, 1688, 1583, 1482, 0, 1847, 0, 1484, 1389, 1392, 0, 1485, 0, 1390, 0, 1484, 1481, 1583, 1634, 0, 1636, 1583, 1243, 1732, 0, 1734, 1846, 1633, 1584, 0, 0, 1584, 5, 1485, 0, 1244, 0, 717, 162, 13, 2111, 0, 1960, 0, 1845, 1731, 1735, 1, 1731, 1583, 1633, 1692, 4, 2222, 2331, 2372, 2331, 2222, 2466, 0, 1536, 0, 1964, 846, 1483, 1585, 1497, 1583, 1633, 1735, 1, 2014, 4, 1816, 2221, 0, 2108, 0, 2108, 0, 2068, 0, 1963, 0, 2064, 2108, 0, 2111, 0, 2014, 1, 1846, 1959, 0, 2065, 1, 2107, 2221, 0, 2111, 0, 2221, 0, 2371, 0, 2221, 0, 2111, 0, 2224, 0, 2065, 0, 2125, 20, 2221, 0, 1743, 1, 2220, 2221, 0, 2463, 0, 2220, 2370, 2276, 2220, 2107, 2009, 1962, 1, 2107, 2224, 4, 2276, 2370, 2276, 2220, 2107, 2107, 2070, 2107, 2013, 0, 2108, 0, 2064, 2107, 2224, 0, 1846, 1958, 2013, 0, 1962, 0, 2020, 2111, 0, 1851, 0, 2231, 9, 2224, 0, 2462, 2369, 2275, 2180, 2009, 2009, 2276, 2220, 2220, 2111, 1, 2107, 2462, 2462, 2370, 2276, 2009, 2109, 2226]\n"
          ]
        }
      ],
      "source": [
        "print(generated_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eunAL6K_9kK7"
      },
      "outputs": [],
      "source": [
        "metadata={\n",
        "    \"ticks_per_beat\": 480,\n",
        "    \"time_signature\": MetaMessage(\"time_signature\", numerator=4, denominator=4),\n",
        "    \"key_signature\": MetaMessage(\"key_signature\", key=\"C\"),\n",
        "    \"set_tempo\": MetaMessage(\"set_tempo\", tempo=500000)\n",
        "}\n",
        "\n",
        "decode_ids_to_midi(generated_ids, \"generated_output.mid\", metadata, id2token)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
