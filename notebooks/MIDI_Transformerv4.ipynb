{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c05fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"SymphonicAI.ipynb\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c688278",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!unzip /content/DATASET_AUGMENTED.zip -d /content/DATASET_AUGMENTED/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef9787",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qq torch midi_neural_processor pretty_midi tqdm utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31967ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "import midi_neural_processor.processor as midi_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c24fe5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Defining model and dataset configuration\"\"\"\n",
    "\n",
    "event_range = midi_tokenizer.RANGE_NOTE_ON\n",
    "event_range += midi_tokenizer.RANGE_NOTE_OFF\n",
    "event_range += midi_tokenizer.RANGE_TIME_SHIFT\n",
    "event_range += midi_tokenizer.RANGE_VEL\n",
    "\n",
    "CONFIG = {\n",
    "    \"max_sequence_len\": 1024,\n",
    "    \"block_size\": 512,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_layers\": 6, #num_blocks\n",
    "    \"batch_size\": 16,\n",
    "    \"token_pad\": event_range,\n",
    "    \"token_sos\": event_range + 1,\n",
    "    \"token_eos\": event_range + 2,\n",
    "    \"vocab_size\": event_range + 3,\n",
    "    \"seed\": 42,\n",
    "    \"model_out\": \"midi_transformer_model.pth\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f474fb0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Splitting dataset into train and evaluation files\"\"\"\n",
    "\n",
    "DATASET_DIR = \"/content/DATASET_AUGMENTED/DATASET_AUGMENTED\"\n",
    "\n",
    "all_midis = [f for f in os.listdir(DATASET_DIR) if f.endswith(\".mid\")]\n",
    "random.shuffle(all_midis)\n",
    "\n",
    "n = len(all_midis) # 35256 files\n",
    "n_train = int(0.9 * n) # 31730 files\n",
    "n_validation = int(0.1 * n) # 3526 files\n",
    "\n",
    "train_files = all_midis[:n_train]\n",
    "validation_files = all_midis[n_train:]\n",
    "\n",
    "print(type(train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19043bd9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "number_of_tokens = []\n",
    "\n",
    "for f in train_files:\n",
    "    tokens = midi_tokenizer.encode_midi(os.path.join(DATASET_DIR, f))\n",
    "    number_of_tokens.append(len(tokens))\n",
    "    if(len(number_of_tokens) == 3000):\n",
    "        break\n",
    "\n",
    "print(number_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc089d4e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"file\": train_files[:3000],\n",
    "    \"token_count\": number_of_tokens\n",
    "})\n",
    "print(df[\"token_count\"].describe())\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(number_of_tokens, bins=50)\n",
    "plt.title(\"Distribution of MIDI Token Counts\")\n",
    "plt.xlabel(\"Number of Tokens\")\n",
    "plt.ylabel(\"Number of Files\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa03290",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"ENCODE EACH MIDI FILE AND DIVIDE IT INTO CHUNKS OF 512 TOKENS WITH SOS, EOS AND PAD\"\"\"\n",
    "\n",
    "list_of_512_seqs = []\n",
    "\n",
    "for f in tqdm.tqdm(all_midis, desc=\"Encoding MIDI files into 512 sequences\"):\n",
    "    tokens = midi_tokenizer.encode_midi(os.path.join(DATASET_DIR, f))\n",
    "\n",
    "    for i in range(0, len(tokens), 510):\n",
    "        chunk = tokens[i : i + 510]\n",
    "\n",
    "        if len(chunk) < 510:\n",
    "            pad_count = 510 - len(chunk)\n",
    "            chunk = chunk + [CONFIG['token_pad']] * pad_count\n",
    "\n",
    "        seq512 = [CONFIG['token_sos']] + chunk + [CONFIG['token_eos']]\n",
    "        assert len(seq512) == 512\n",
    "\n",
    "        list_of_512_seqs.append(seq512)\n",
    "\n",
    "print(f\"Total 512-token sequences: {len(list_of_512_seqs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1655f718",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "n = len(list_of_512_seqs)\n",
    "print(n)\n",
    "\n",
    "train_seqs = list_of_512_seqs[:int(0.9 * n)]\n",
    "val_seqs = list_of_512_seqs[int(0.9 * n):]\n",
    "\n",
    "print(len(train_seqs))\n",
    "print(len(val_seqs))\n",
    "\n",
    "print(len(list_of_512_seqs))\n",
    "print(list_of_512_seqs[0])\n",
    "print(list_of_512_seqs[1])\n",
    "print(list_of_512_seqs[2])\n",
    "\n",
    "print(type(train_seqs))\n",
    "print(len(train_seqs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc7f2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5956467a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_seqs, dtype=torch.long, device=device)\n",
    "print(train_data.shape, train_data.dtype)\n",
    "print(train_data[:100])\n",
    "\n",
    "test_data = torch.tensor(val_seqs, dtype=torch.long, device=device)\n",
    "print(test_data.shape, test_data.dtype)\n",
    "print(test_data[:100])\n",
    "\n",
    "X_train = train_data[:, :-1]\n",
    "Y_train = train_data[:, 1:]\n",
    "\n",
    "X_val = test_data[:, :-1]\n",
    "Y_val = test_data[:, 1:]\n",
    "\n",
    "train_ds = TensorDataset(X_train, Y_train)\n",
    "val_ds   = TensorDataset(X_val,   Y_val)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb7120",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei = q @ k.transpose(-2, -1) / (k.size(-1) ** 0.5)\n",
    "\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        return self.dropout(wei @ v)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, n_embd, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // num_heads\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.proj(torch.cat([h(x) for h in self.heads], dim=-1)))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(n_head, n_embd, block_size, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerMIDILanguageModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size    = config['block_size']\n",
    "        self.vocab_size    = config['vocab_size']\n",
    "        self.sos_token_id  = config['token_sos']\n",
    "        self.pad_token_id  = config['token_pad']\n",
    "        self.eos_token_id  = config['token_eos']\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(\n",
    "            self.vocab_size,\n",
    "            config['embedding_dim'],\n",
    "            padding_idx=self.pad_token_id\n",
    "        )\n",
    "\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, config['embedding_dim'])\n",
    "\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(n_embd=config['embedding_dim'],\n",
    "                  n_head=config['num_heads'],\n",
    "                  block_size=config['block_size'],\n",
    "                  dropout=0.1)\n",
    "            for _ in range(config['num_layers'])\n",
    "            ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(config['embedding_dim'])\n",
    "        self.lm_head = nn.Linear(config['embedding_dim'], self.vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "\n",
    "        x = self.blocks(tok_emb + pos_emb)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1), ignore_index=self.pad_token_id)\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, eos_token_id, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k, dim=-1)\n",
    "                min_vals = v[:, -1].unsqueeze(1)\n",
    "                logits[logits < min_vals] = -float('Inf')\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "            if eos_token_id is not None:\n",
    "                # check every item in batch\n",
    "                eos_mask = idx_next.eq(eos_token_id).view(-1)\n",
    "                if eos_mask.all():\n",
    "                    break\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec59e5c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = TransformerMIDILanguageModel(CONFIG).to(device)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c4989",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        losses = []\n",
    "        for i, (xb, yb) in enumerate(loader):\n",
    "            if i>= eval_iters:\n",
    "                break\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ddfe3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "del iter\n",
    "learning_rate = 1e-4\n",
    "max_iters = 50000\n",
    "eval_interval = 100\n",
    "eval_iters = 200\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabca408",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for step in range(max_iters):\n",
    "    if step % eval_interval == 0 or step == max_iters -1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Iteration {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    try:\n",
    "        xb, yb = next(train_iterator)\n",
    "    except StopIteration:\n",
    "        train_iterator = iter(train_loader)\n",
    "        xb, yb = next(train_iterator)\n",
    "\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    _, loss = model(xb, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "torch.save(model.state_dict(), CONFIG['model_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935ef90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = TransformerMIDILanguageModel(CONFIG).to(device)\n",
    "model.load_state_dict(torch.load(CONFIG['model_out']))\n",
    "model.eval()\n",
    "\n",
    "# Generate a new MIDI sequence\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Start with an empty context\n",
    "context[0, 0] = CONFIG['token_sos']  # Start with SOS token\n",
    "generated_sequence = model.generate(context, max_new_tokens=512, eos_token_id=CONFIG['token_eos'], temperature=1.0)\n",
    "\n",
    "# Convert the generated sequence back to MIDI data\n",
    "generated_midi = midi_tokenizer.decode_midi(generated_sequence[0].tolist())\n",
    "\n",
    "# Save the generated MIDI file\n",
    "output_midi_path = \"generated_midi.mid\"\n",
    "generated_midi.write(output_midi_path)\n",
    "print(f\"Generated MIDI file saved to: {output_midi_path}\")\n",
    "\n",
    "# prompt: give the model MIDI context (list of tokens) and make it generate its continuation\n",
    "\n",
    "# Assuming 'generated_sequence' is already defined from the previous code\n",
    "\n",
    "# Example context (replace with your actual MIDI context)\n",
    "# context_tokens = [CONFIG['token_sos'], 10, 25, 50, 75] # Example tokens, replace with your actual MIDI data\n",
    "\n",
    "context_tokens_fur_elise = [CONFIG['token_sos'], 342, 365, 76, 277, 204, 364, 75, 277, 203, 365, 76, 276, 204, 365, 75, 276, 203, 367, 76, 276, 204, 366, 71, 277, 199, 367, 74, 277, 202, 365, 72, 279, 200, 365, 45, 368, 69, 277, 173, 364, 52, 277, 197, 364, 57, 277, 364, 60, 276, 365, 64, 276, 366, 69, 278, 367, 71, 366, 40, 258, 180, 185, 188, 192, 197, 273, 168, 364, 52, 277, 199, 364, 56, 276, 365, 64, 276, 365, 68, 276, 366, 71, 279, 367, 72, 365, 45, 256, 180, 184, 192, 196, 199, 276, 173, 364, 52, 277, 200, 364, 57, 277, 364, 64, 261, 180, 185, 271, 192, 365, 76, 277, 204, 364, 75, 276, 203, 366, 76, 276, 204, 365, 75, 276, 203, 367, 76, 276, 204, 365, 71, 277, 199, 368, 74, 277, 202, 365, 72, 278, 200, 365, 45, 368, 69, 277, 173, 364, 52, 277, 197, 364, 57, 277, 364, 60, 276, 365, 64, 276, 366, 69, 278, 367, 71, 365, 40, 257, 180, 185, 188, 192, 197, 275, 168, 364, 52, 277, 199, 365, 56, 276, 365, 64, 277, 366, 72, 277, 365, 71, 278, 366, 69, 365, 45, 258, 180, 184, 192, 200, 199, 275, 173, 364, 52, 278, 197, 364, 57, 283, 180, 185, 277, 365, 76, 278, 204, 364, 75, 277, 203, 365, 76, 276, 204, 365, 75, 276, 203, 367, 76, 277, 204, 366, 71, 276, 199, 367, 74, 276, 202, 365, 72, 279, 200, 365, 45, 368, 69, 277, 173, 364, 52, 277, 197, 364, 57, 277, 364, 60, 276, 365, 64, 276, 366, 69, 278, 367, 71, 366, 40, 258, 180, 185, 188, 192, 197, 273, 168, 364, 52, 277, 199, 364, 56, 276, 365, 64, 276, 365, 68, 276, 366, 71, 279, 367, 72, 365, 45, 256, 180, 184, 192, 196, 199, 276, 173, 364, 52, 277, 200, 364, 57, 277, 364, 64, 261, 180, 185, 271, 192, 365, 76, 277, 204, 364, 75, 276, 203, 366, 76, 276, 204, 365, 75, 276, 203, 367, 76, 276, 204, 365, 71, 277, 199, 368, 74, 277, 202, 365, 72, 278, 200, 365, 45, 368, 69, 277, 173, 364, 52, 277, 197, 364, 57, 277, 364, 60, 276, 365, 64, 276, 366, 69, 278, 367, 71, 365, 40, 257, 180, 185, 188, 192, 197, 275, 168, 365, 52, 277, 199, 365, 56, 277, 365, 64, 278, 366, 72, 278, 365, 71, 278, 366, 69, 365, 45, 259, 180, 184, 192, 200, 199, 275, 173, 364, 52, 278, 197, 364, 57, 277, 366, 71, 277, 367, 72, 277, 368, 74, 278, 368, 76, 367, 48, 257, 180, 185, 199, 200, 202, 274, 176, 365, 55, 276, 365, 60, 277, 204, 366, 67, 277, 367, 77, 277, 366, 76, 277, 368, 74, 367, 43, 257, 183, 188, 195, 205, 204, 274, 171, 365, 55, 276, 365, 59, 277, 202, 366, 65, 277, 367, 76, 277, 366, 74, 277, 368, 72, 367, 45, 257, 183, 187, 193, 204, 202, 274, 173, 365, 52, 276, 365, 57, 277, 200, 366, 64, 278, 367, CONFIG['token_eos']]\n",
    "\n",
    "context = torch.tensor([context_tokens_fur_elise], dtype=torch.long, device=device)\n",
    "\n",
    "# Generate continuation\n",
    "generated_sequence = model.generate(context, max_new_tokens=512, eos_token_id=CONFIG['token_eos'], temperature=1.0)\n",
    "\n",
    "# Convert the generated sequence back to MIDI data\n",
    "generated_midi = midi_tokenizer.decode_midi(generated_sequence[0].tolist())\n",
    "\n",
    "# Save the generated MIDI file\n",
    "output_midi_path = \"generated_midi_from_furelise.mid\"\n",
    "generated_midi.write(output_midi_path)\n",
    "print(f\"Generated MIDI file saved to: {output_midi_path}\")\n",
    "\n",
    "first_tokens = [280, 376, 63, 286, 191, 376, 63, 286, 191, 376, 63, 286, 191, 376, 63, 302, 191, 376, 65, 302, 193, 376, 62, 317, 190, 376, 62, 286, 190, 376, 62, 286, 190, 376, 62, 286, 190, 376, 62, 302, 190, 376, 63, 302, 191, 376, 60, 317, 188, 376, 60, 286, 188, 376, 60, 286, 188, 376, 60, 286, 188, 376, 60, 302, 188, 376, 55, 302, 183, 376, 56, 317, 184, 376, 60, 286, 188, 376, 60, 286, 188, 376, 60, 286, 188, 376, 60, 302, 188, 376, 62, 302, 190, 376, 63, 355, 311, 191, 286, 376, 51, 286, 179, 376, 51, 286, 179, 376, 58, 349, 186, 376, 55, 286, 183, 376, 53, 286, 181, 376, 53, 286, 181, 376, 51, 286, 179, 376, 51, 286, 179, 376, 58, 349, 186, 376, 55, 286, 183, 376, 53, 286, 181, 376, 53, 286, 181, 376, 51, 286, 179, 376, 51, 286, 179, 376, 60, 349, 188, 376, 55, 286, 183, 376, 53, 286, 181, 376, 53, 286, 181, 376, 51, 286, 179, 376, 51, 286, 179, 376, 56, 317, 184, 376, 56, 317, 184, 376, 56, 317, 184, 376, 51, 286, 179, 376, 51, 286, 179, 376, 58, 349, 186, 376, 55, 286, 183, 376, 53, 286, 181, 376, 53, 286, 181, 376, 51, 286, 179, 376, 51, 286, 179, 376, 58, 349, 186, 376, 55, 286, 183, 376, 53, 286, 181, 376, 53, 286, 181, 376, 51, 286, 179, 376, 51, 286, 179, 376, 60, 349, 188, 376, 55, 286, 183, 376, 53, 286, 181, 376, 53, 286, 181, 376, 51, 286, 179, 376, 51, 286, 179, 376, 56, 317, 184, 376, 56, 317, 184, 376, 56, 286, 184, 376, 55, 355, 280, 183, 286, 376, 58, 286, 186, 376, 58, 286, 186, 376, 58, 286, 186, 376, 58, 302, 186, 376, 55, 302, 183, 376, 58, 317, 186, 376, 58, 286, 186, 376, 58, 286, 186, 376, 58, 286, 186, 376, 63, 302, 191, 376, 62, 302, 190, 376, 60, 317, 188, 376, 55, 286, 183, 376, 55, 286, 183, 376, 55, 286, 183, 376, 55, 302, 183, 376, 55, 302, 183, 376, 56, 317, 184, 376, 56, 286, 184, 376, 56, 286, 184, 376, 56, 286, 184, 376, 56, 302, 184, 376, 55, 302, 183, 376, 55, 317, 183, 376, 63, 286, 191, 376, 63, 286, 191, 376, 63, 286, 191, 376, 63, 302, 191, 376, 63, 302, 191, 376, 62, 317, 190, 376, 62, 286, 190, 376, 62, 286, 190, 376, 62, 286, 190, 376, 62, 302, 190, 376, 63, 302, 191, 376, 60, 317, 188, 376, 55, 286, 183, 376, 55, 286, 183, 376, 55, 286, 183, 376, 55, 302, 183, 376, 55, 302, 183, 376, 56, 317, 184, 376, 56, 286, 184, 376, 56, 286, 184, 376, 56, 286, 184, 376, 56, 302, 184, 376, 55, 302, 183, 376, 55, 349, 183, 384, 67, 286, 195, 384, 67, 286, 195, 384, 70, 302, 198, 384, 63, 302, 191, 384, 62, 349, 190, 384, 67, 286, 195, 384, 67, 286, 195, 384]\n",
    "context_demons_imagine_dragons_tokens = [CONFIG['token_sos'],\n",
    "                                         *first_tokens,\n",
    "                                         CONFIG['token_eos']]\n",
    "\n",
    "context = torch.tensor([context_demons_imagine_dragons_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "# Generate continuation\n",
    "generated_sequence = model.generate(context, max_new_tokens=512, eos_token_id=CONFIG['token_eos'], temperature=1.0)\n",
    "\n",
    "# Convert the generated sequence back to MIDI data\n",
    "generated_midi = midi_tokenizer.decode_midi(generated_sequence[0].tolist())\n",
    "\n",
    "# Save the generated MIDI file\n",
    "output_midi_path = \"generated_midi_from_demons.mid\"\n",
    "generated_midi.write(output_midi_path)\n",
    "print(f\"Generated MIDI file saved to: {output_midi_path}\")\n",
    "\n",
    "first_tokens = [275, 376, 70, 275, 198, 376, 70, 275, 198, 376, 70, 275, 198, 376, 70, 285, 198, 376, 72, 285, 200, 376, 69, 295, 197, 376, 69, 275, 197, 376, 69, 275, 197, 376, 69, 275, 197, 376, 69, 285, 197, 376, 70, 285, 198, 376, 67, 295, 195, 376, 67, 275, 195, 376, 67, 275, 195, 376, 67, 275, 195, 376, 67, 285, 195, 376, 62, 285, 190, 376, 63, 295, 191, 376, 67, 275, 195, 376, 67, 275, 195, 376, 67, 275, 195, 376, 67, 285, 195, 376, 69, 285, 197, 376, 70, 355, 198, 275, 376, 58, 275, 186, 376, 58, 275, 186, 376, 65, 315, 193, 376, 62, 275, 190, 376, 60, 275, 188, 376, 60, 275, 188, 376, 58, 275, 186, 376, 58, 275, 186, 376, 65, 315, 193, 376, 62, 275, 190, 376, 60, 275, 188, 376, 60, 275, 188, 376, 58, 275, 186, 376, 58, 275, 186, 376, 67, 315, 195, 376, 62, 275, 190, 376, 60, 275, 188, 376, 60, 275, 188, 376, 58, 275, 186, 376, 58, 275, 186, 376, 63, 295, 191, 376, 63, 295, 191, 376, 63, 295, 191, 376, 58, 275, 186, 376, 58, 275, 186, 376, 65, 315, 193, 376, 62, 275, 190, 376, 60, 275, 188, 376, 60, 275, 188, 376, 58, 275, 186, 376, 58, 275, 186, 376, 65, 315, 193, 376, 62, 275, 190, 376, 60, 275, 188, 376, 60, 275, 188, 376, 58, 275, 186, 376, 58, 275, 186, 376, 67, 315, 195, 376, 62, 275, 190, 376, 60, 275, 188, 376, 60, 275, 188, 376, 58, 275, 186, 376, 58, 275, 186, 376, 63, 295, 191, 376, 63, 295, 191, 376, 63, 275, 191, 376, 62, 335, 190, 275, 376, 65, 275, 193, 376, 65, 275, 193, 376, 65, 275, 193, 376, 65, 285, 193, 376, 62, 285, 190, 376, 65, 295, 193, 376, 65, 275, 193, 376, 65, 275, 193, 376, 65, 275, 193, 376, 70, 285, 198, 376, 69, 285, 197, 376, 67, 295, 195, 376, 62, 275, 190, 376, 62, 275, 190, 376, 62, 275, 190, 376, 62, 285, 190, 376, 62, 285, 190, 376, 63, 295, 191, 376, 63, 275, 191, 376, 63, 275, 191, 376, 63, 275, 191, 376, 63, 285, 191, 376, 62, 285, 190, 376, 62, 295, 190, 376, 70, 275, 198, 376, 70, 275, 198, 376, 70, 275, 198, 376, 70, 285, 198, 376, 70, 285, 198, 376, 69, 295, 197, 376, 69, 275, 197, 376, 69, 275, 197, 376, 69, 275, 197, 376, 69, 285, 197, 376, 70, 285, 198, 376, 67, 295, 195, 376, 62, 275, 190, 376, 62, 275, 190, 376, 62, 275, 190, 376, 62, 285, 190, 376, 62, 285, 190, 376, 63, 295, 191, 376, 63, 275, 191, 376, 63, 275, 191, 376, 63, 275, 191, 376, 63, 285, 191, 376, 62, 285, 190, 376, 62, 315, 190, 384, 74, 275, 202, 384, 74, 275, 202, 384, 77, 285, 205, 384, 70, 285, 198, 384, 69, 315, 197, 384, 74, 275, 202, 384, 74, 275, 202, 384, 77, 285]\n",
    "context_demons_imagine_dragons_fast_tokens = [CONFIG['token_sos'],\n",
    "                                         *first_tokens,\n",
    "                                         CONFIG['token_eos']]\n",
    "\n",
    "context = torch.tensor([context_demons_imagine_dragons_fast_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "# Generate continuation\n",
    "generated_sequence = model.generate(context, max_new_tokens=512, eos_token_id=CONFIG['token_eos'], temperature=1.0)\n",
    "\n",
    "# Convert the generated sequence back to MIDI data\n",
    "generated_midi = midi_tokenizer.decode_midi(generated_sequence[0].tolist())\n",
    "\n",
    "# Save the generated MIDI file\n",
    "output_midi_path = \"generated_midi_from_fast_demons.mid\"\n",
    "generated_midi.write(output_midi_path)\n",
    "print(f\"Generated MIDI file saved to: {output_midi_path}\")\n",
    "\n",
    "first_tokens = [376, 58, 315, 186, 376, 60, 275, 188, 376, 58, 295, 186, 376, 57, 295, 185, 376, 55, 295, 183, 376, 57, 295, 185, 376, 58, 295, 186, 376, 65, 295, 193, 376, 65, 335, 193, 335, 376, 62, 295, 190, 376, 63, 295, 191, 376, 62, 335, 190, 376, 58, 315, 186, 376, 60, 275, 188, 376, 58, 295, 186, 376, 57, 295, 185, 376, 55, 295, 183, 376, 57, 295, 185, 376, 58, 295, 186, 376, 65, 295, 193, 376, 65, 335, 193, 335, 376, 62, 295, 190, 376, 63, 295, 191, 376, 62, 335, 190, 376, 57, 315, 185, 376, 58, 275, 186, 376, 60, 295, 188, 376, 57, 295, 185, 376, 58, 335, 186, 355, 355, 355, 355, 376, 58, 315, 186, 376, 60, 275, 188, 376, 58, 295, 186, 376, 57, 295, 185, 376, 55, 295, 183, 376, 57, 295, 185, 376, 58, 295, 186, 376, 65, 295, 193, 376, 65, 335, 193, 335, 376, 65, 295, 193, 376, 65, 295, 193, 376, 65, 335, 193, 376, 58, 315, 186, 376, 60, 275, 188, 376, 58, 295, 186, 376, 57, 295, 185, 376, 55, 295, 183, 376, 57, 295, 185, 376, 58, 295, 186, 376, 65, 295, 193, 376, 65, 335, 193, 335, 376, 62, 295, 190, 376, 63, 295, 191, 376, 62, 335, 190, 376, 58, 315, 186, 376, 60, 275, 188, 376, 58, 295, 186, 376, 57, 295, 185, 376, 55, 295, 183, 376, 57, 295, 185, 376, 58, 295, 186, 376, 65, 295, 193, 376, 65, 335, 193, 335, 376, 62, 295, 190, 376, 63, 295, 191, 376, 62, 335, 190, 376, 57, 315, 185, 376, 58, 275, 186, 376, 60, 295, 188, 376, 57, 295, 185, 376, 58, 335, 186, 355, 355, 355, 355, 376, 58, 315, 186, 376, 60, 275, 188, 376, 58, 295, 186, 376, 57, 295, 185, 376, 55, 295, 183, 376, 57, 295, 185, 376, 58, 295, 186, 376, 65, 295, 193, 376, 65, 335, 193, 335, 376, 65, 295, 193, 376, 65, 295, 193, 376, 65, 335, 193]\n",
    "context_demons_low = [CONFIG['token_sos'],\n",
    "                                         *first_tokens,\n",
    "                                         CONFIG['token_eos']]\n",
    "\n",
    "context = torch.tensor([context_demons_low], dtype=torch.long, device=device)\n",
    "\n",
    "# Generate continuation\n",
    "generated_sequence = model.generate(context, max_new_tokens=512, eos_token_id=CONFIG['token_eos'], temperature=1.0)\n",
    "\n",
    "# Convert the generated sequence back to MIDI data\n",
    "generated_midi = midi_tokenizer.decode_midi(generated_sequence[0].tolist())\n",
    "\n",
    "# Save the generated MIDI file\n",
    "output_midi_path = \"generated_midi_from_low.mid\"\n",
    "generated_midi.write(output_midi_path)\n",
    "print(f\"Generated MIDI file saved to: {output_midi_path}\")\n",
    "\n",
    "print(generated_sequence)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
